---
# Neural collapse config file. All must be specified
Model:
  model-name: None            # Name of model in "our_models"
  embedding-layers: [
    conv, fc
  ]     # Intermediate layers to use for NC loss with weightings (True=>All layers/default)

Data:
  dataset-id: None       # Which dataset-getter to use. Note that shapes are provided by the dataset
  batch-size: 128        # Mini-batch size
  do-augmentation: False # Whether to do data augmentation

Optimizer:
  loss: mseloss
  weight-decay: None    # Weight decay
  lr: None              # Learning rate
  lr-decay: 0.2
  lr-decay-steps: 3      # Number of learning rate decay steps
  momentum: 0.9          # Optimizer momentum
  epochs: 300            # Number of epochs to train for

Logging:
  # When to store weights and calculate measurements
  save-dir: logs/matrix/convnet_huge
  log-interval: 25        # At what interval to log checkpoints. Always includes first 10 epochs
  # log-epochs: [0, 1, 2, 3, 4, 5, 7, 10, 14, 20, 30, 40, 50, 60, 80, 100, 125, 150, 175, 200, 225, 250, 275, 300]  # Overrides log-interval

Measurements:
  measures: Fast

Matrix:
  Model:
    model-name: [convnet_huge]
  Data:
    dataset-id: [MNIST, cifar10]
  Optimizer:
    lr: [0.01, 0.0025]
    weight-decay: [5.e-4, 1.e-4]
...
